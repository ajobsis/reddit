{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d21474a",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5173643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                     cross_val_score)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (RandomForestClassifier, \n",
    "                                BaggingClassifier)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from  nltk.stem import PorterStemmer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7dc5a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, name, X, y):\n",
    "    cv = StratifiedKFold(n_splits=10,\n",
    "                         random_state=73,\n",
    "                         shuffle=True)\n",
    "    s = cross_val_score(model, X, y, cv=cv,\n",
    "                        n_jobs=-1)\n",
    "    print('{} Score: {:.2f} +- {:.3f}'.format(name, \n",
    "                                              s.mean(), \n",
    "                                              2 * s.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73996c1",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a61808c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "reddits = pd.read_csv('../data/reddit_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "368629d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target\n",
    "reddits['target'] = reddits.num_comments.\\\n",
    "    apply(lambda x: 0 if x < reddits.num_comments.median() else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c9b6a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5085933045532296"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline 0.5085933045532296\n",
    "reddits.target.value_counts(normalize=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "725d5ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.6, max_features=200, stop_words='english',\n",
       "                strip_accents='ascii')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate vectorizer\n",
    "tvec = TfidfVectorizer(stop_words = 'english',\n",
    "                       strip_accents = 'ascii',\n",
    "                       max_features = 200,\n",
    "                       max_df = .60,\n",
    "                       norm = 'l2'\n",
    "                      )\n",
    "tvec.fit(reddits.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1463c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorized dataframe\n",
    "reddits_tvec = pd.DataFrame(tvec.transform(reddits.title).todense(),\n",
    "                  columns = tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49447740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat vectorized and reddit dataframes\n",
    "reddits_vect = pd.concat([reddits,reddits_tvec],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c16030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiate X and y\n",
    "X = reddits_vect[['upvote_ratio', 'length_time','score',\n",
    "        'num_words', 'title_length','depp','ukraine','school','police','gun',\n",
    "        'amber','pride','man','2022','gun', 'new','year','years',\n",
    "        'today','season','russian','hate','spoilers','free','buy',\n",
    "        'actually','work','life','fuck','shares','dog']]\n",
    "\n",
    "y = reddits_vect.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62862927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=.2,\n",
    "                                                   random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eae868ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scale the data\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train, y_train)\n",
    "X_train_scaled = ss.transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717ad50",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "afb8c404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, random_state=73, solver='liblinear')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear',\n",
    "                       C=100,\n",
    "                       penalty='l2',\n",
    "                       random_state=73\n",
    "                       )\n",
    "\n",
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d5c9ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Score: 0.72 +- 0.031\n"
     ]
    }
   ],
   "source": [
    "# score X_train\n",
    "score(lr, 'Logistic regression', X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca4b31a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Score: 0.71 +- 0.053\n"
     ]
    }
   ],
   "source": [
    "# score X_test\n",
    "score(lr, 'Logistic regression', X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc7b3e",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97583045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(min_samples_leaf=5, n_estimators=200, n_jobs=-1,\n",
       "                       random_state=73, warm_start=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate and fit\n",
    "rf = RandomForestClassifier(n_jobs=-1,\n",
    "                            random_state=73,\n",
    "                            n_estimators=200,\n",
    "                            min_samples_leaf=5,\n",
    "                            warm_start=True)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db10ffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest Score: 0.74 +- 0.032\n"
     ]
    }
   ],
   "source": [
    "# score the X_train\n",
    "score(rf, 'Random forest', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4fb5b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest Score: 0.71 +- 0.046\n"
     ]
    }
   ],
   "source": [
    "# score the X_test\n",
    "score(rf, 'Random forest', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ea512",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "03512e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate \n",
    "dt = DecisionTreeClassifier(random_state=73,\n",
    "                            class_weight='balanced',)\n",
    "bdt = BaggingClassifier(dt,\n",
    "                        random_state=73,\n",
    "                        n_jobs=-1,\n",
    "                        n_estimators=200)\n",
    "\n",
    "bdt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae26beee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Score: 0.72 +- 0.028\n"
     ]
    }
   ],
   "source": [
    "# score X_train\n",
    "score(bdt, 'Bagging', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2f1c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Score: 0.71 +- 0.037\n"
     ]
    }
   ],
   "source": [
    "# score X_test\n",
    "score(bdt, 'Bagging', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080034b",
   "metadata": {},
   "source": [
    "### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5d2e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6b0986dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddits_stem = reddits['title'].apply(lambda x: stemmer.stem(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8573fd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        banned from my street on friday is this allow\n",
       "1    rift in this weeks iron banner is inherently n...\n",
       "2               nadal5 defeats djokovic 1 62 46 62 764\n",
       "3                                   what would you say\n",
       "4                                             mosh pit\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddits_stem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a2e3d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "de5e0d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Banned from my street on Friday Is this allowed\n",
       "1        Rift in this weeks Iron Banner is inherently n...\n",
       "2                   Nadal5 defeats Djokovic 1 62 46 62 764\n",
       "3                                       What would you say\n",
       "4                                                 Mosh pit\n",
       "                               ...                        \n",
       "12272                                                  100\n",
       "12273                            Still Early But Not Wrong\n",
       "12274          WuTang is for the children its for Apes too\n",
       "12275    Made an alternate version of this meme templat...\n",
       "12276    Keep digging hedgies U R FUK BUY HODL DRS Lock...\n",
       "Name: title, Length: 12277, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddits.title.apply(lambda x: lem.lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d9566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49844d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddits_greater = reddits_vect[reddits_vect.target == 1].drop(['author', \n",
    "            'title','created', 'subreddit', 'num_comments',\n",
    "            'score', 'upvote_ratio', 'length_time',\n",
    "            'num_words', 'title_length', 'target'], axis=1).sum().to_frame('count1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddits_lesser = reddits_vect[reddits_vect.target == 0].drop(['author', \n",
    "            'title','created', 'subreddit', 'num_comments',\n",
    "            'score', 'upvote_ratio', 'length_time',\n",
    "            'num_words', 'title_length', 'target'], axis=1).sum().to_frame('count0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat((reddits_greater, reddits_lesser), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.sort_values('count0', ascending = False)\n",
    "\n",
    "test['diff'] = test.count1 - test.count0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50fbf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sort_values('diff', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc071ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c25cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
